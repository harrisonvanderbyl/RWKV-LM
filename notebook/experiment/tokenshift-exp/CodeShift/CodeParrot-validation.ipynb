{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the following packages are installed\n",
    "# !pip install transformers datasets tqdm torch transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/picocreator/anaconda3/envs/rwkv-infctx/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-29 10:00:47,497] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/picocreator/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-valid-826c6fd8b27e5523/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "100%|██████████| 1/1 [00:00<00:00, 410.68it/s]\n",
      "Loading cached processed dataset at /home/picocreator/.cache/huggingface/datasets/codeparrot___json/codeparrot--codeparrot-clean-valid-826c6fd8b27e5523/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fc62de51fe8a734f.arrow\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All objects 61373 ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 01:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 - batch loss: 2.597266912460327 - avg loss: 2.597266912460327   (start: 0, end: 10)\n",
      "Batch 1 - batch loss: 2.320502758026123 - avg loss: 2.458884835243225   (start: 10, end: 20)\n",
      "Batch 2 - batch loss: 2.331798553466797 - avg loss: 2.416522741317749   (start: 20, end: 30)\n",
      "Batch 3 - batch loss: 2.1776061058044434 - avg loss: 2.3567935824394226   (start: 30, end: 40)\n",
      "Batch 4 - batch loss: 2.3772802352905273 - avg loss: 2.3608909130096434   (start: 40, end: 50)\n",
      "Batch 5 - batch loss: 2.246580123901367 - avg loss: 2.3418391148249307   (start: 50, end: 60)\n",
      "Batch 6 - batch loss: 2.3905882835388184 - avg loss: 2.3488032817840576   (start: 60, end: 70)\n",
      "Batch 7 - batch loss: 2.9608523845672607 - avg loss: 2.425309419631958   (start: 70, end: 80)\n",
      "Batch 8 - batch loss: 2.7365193367004395 - avg loss: 2.459888299306234   (start: 80, end: 90)\n",
      "Batch 9 - batch loss: 2.8640105724334717 - avg loss: 2.5003005266189575   (start: 90, end: 100)\n",
      "Batch 10 - batch loss: 1.7566168308258057 - avg loss: 2.4326929179104892   (start: 100, end: 110)\n",
      "Batch 11 - batch loss: 2.1285345554351807 - avg loss: 2.4073463877042136   (start: 110, end: 120)\n",
      "Batch 12 - batch loss: 1.5817415714263916 - avg loss: 2.343838324913612   (start: 120, end: 130)\n",
      "Batch 13 - batch loss: 2.8169753551483154 - avg loss: 2.3776338270732333   (start: 130, end: 140)\n",
      "Batch 14 - batch loss: 1.6623344421386719 - avg loss: 2.3299472014109295   (start: 140, end: 150)\n",
      "Batch 15 - batch loss: 2.0510668754577637 - avg loss: 2.3125171810388565   (start: 150, end: 160)\n",
      "Batch 16 - batch loss: 1.8603763580322266 - avg loss: 2.2859206620384667   (start: 160, end: 170)\n",
      "Batch 17 - batch loss: 1.8565582036972046 - avg loss: 2.2620671921306186   (start: 170, end: 180)\n",
      "Batch 18 - batch loss: 2.834724187850952 - avg loss: 2.292207034010636   (start: 180, end: 190)\n",
      "Batch 19 - batch loss: 1.8843004703521729 - avg loss: 2.271811705827713   (start: 190, end: 200)\n",
      "Batch 20 - batch loss: 2.4651548862457275 - avg loss: 2.2810185239428566   (start: 200, end: 210)\n",
      "Batch 21 - batch loss: 2.2990825176239014 - avg loss: 2.2818396145647224   (start: 210, end: 220)\n",
      "Batch 22 - batch loss: 2.5361664295196533 - avg loss: 2.2928973021714585   (start: 220, end: 230)\n",
      "Batch 23 - batch loss: 2.1774022579193115 - avg loss: 2.288085008660952   (start: 230, end: 240)\n",
      "Batch 24 - batch loss: 3.0865261554718018 - avg loss: 2.3200226545333864   (start: 240, end: 250)\n",
      "Batch 25 - batch loss: 2.643110990524292 - avg loss: 2.332449128994575   (start: 250, end: 260)\n",
      "Batch 26 - batch loss: 3.0995936393737793 - avg loss: 2.360861888638249   (start: 260, end: 270)\n",
      "Batch 27 - batch loss: 1.476658582687378 - avg loss: 2.329283199140004   (start: 270, end: 280)\n",
      "Batch 28 - batch loss: 2.7185158729553223 - avg loss: 2.342705015478463   (start: 280, end: 290)\n",
      "Batch 29 - batch loss: 1.700026273727417 - avg loss: 2.3212823907534283   (start: 290, end: 300)\n",
      "Batch 30 - batch loss: 1.3138316869735718 - avg loss: 2.288783980954078   (start: 300, end: 310)\n",
      "Batch 31 - batch loss: 2.012782096862793 - avg loss: 2.2801589220762253   (start: 310, end: 320)\n",
      "Batch 32 - batch loss: 1.5903985500335693 - avg loss: 2.259257092620387   (start: 320, end: 330)\n",
      "Batch 33 - batch loss: 2.0468859672546387 - avg loss: 2.2530108830508064   (start: 330, end: 340)\n",
      "Batch 34 - batch loss: 1.9905130863189697 - avg loss: 2.245510946001325   (start: 340, end: 350)\n",
      "Batch 35 - batch loss: 2.491105079650879 - avg loss: 2.2523330052693686   (start: 350, end: 360)\n",
      "Batch 36 - batch loss: 1.7607126235961914 - avg loss: 2.23904596792685   (start: 360, end: 370)\n",
      "Batch 37 - batch loss: 2.1389946937561035 - avg loss: 2.236413039659199   (start: 370, end: 380)\n",
      "Batch 38 - batch loss: 2.1325459480285645 - avg loss: 2.2337497808994393   (start: 380, end: 390)\n",
      "Batch 39 - batch loss: 1.6011826992034912 - avg loss: 2.2179356038570406   (start: 390, end: 400)\n",
      "Batch 40 - batch loss: 2.673210620880127 - avg loss: 2.2290398725649205   (start: 400, end: 410)\n",
      "Batch 41 - batch loss: 2.3977532386779785 - avg loss: 2.2330568574723744   (start: 410, end: 420)\n",
      "Batch 42 - batch loss: 2.7680413722991943 - avg loss: 2.245498357817184   (start: 420, end: 430)\n",
      "Batch 43 - batch loss: 2.3662502765655518 - avg loss: 2.2482427196069197   (start: 430, end: 440)\n",
      "Batch 44 - batch loss: 1.5410813093185425 - avg loss: 2.2325280216005114   (start: 440, end: 450)\n",
      "Batch 45 - batch loss: 2.0603346824645996 - avg loss: 2.228784688141035   (start: 450, end: 460)\n",
      "Batch 46 - batch loss: 2.1261138916015625 - avg loss: 2.2266002031082803   (start: 460, end: 470)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, default_data_collator \n",
    "from datasets import load_dataset\n",
    "import torch, gc, os, sys, copy\n",
    "\n",
    "# Disable WANDB\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "# Lets clear out items and GC\n",
    "tokenizer = None\n",
    "eval_dataset = None\n",
    "model = None\n",
    "trainer = None\n",
    "\n",
    "# Clear everything!\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('codeparrot/codeparrot', use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "# Load 'codeparrot/codeparrot-clean-valid' Dataset\n",
    "eval_dataset = load_dataset('codeparrot/codeparrot-clean-valid')[\"train\"]\n",
    "\n",
    "# # Reduce top level dataset, to speed up debug iteration\n",
    "# eval_dataset = eval_dataset.select(range(0, 200, 1))\n",
    "\n",
    "# Tokenize content to input_ids, attention_mask, and labels\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"content\"], truncation=True, max_length=1024, \n",
    "        padding='max_length', return_attention_mask=True\n",
    "    ) \n",
    "\n",
    "    # Shift the input_ids one position to the right to create the labels.\n",
    "    tokenized[\"labels\"] = copy.deepcopy(tokenized[\"input_ids\"][1:1024])\n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"][:1023]\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"][:1023]\n",
    "\n",
    "    # Tokenized output\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize the content datasets\n",
    "eval_dataset = eval_dataset.map(tokenize_function, batched=False, remove_columns=[\"content\"])\n",
    "eval_dataset.set_format(type='torch', columns=[\"labels\",'input_ids', 'attention_mask'])\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    # No gradient\n",
    "    with torch.no_grad():\n",
    "        # Logits and labels batches\n",
    "        logits_batch, labels_batch = eval_pred\n",
    "\n",
    "        # Batch size\n",
    "        batch_size = logits_batch.shape[0]\n",
    "        # print(\"BATCH_SIZE??\", batch_size)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = 0\n",
    "\n",
    "        # Looks like i gotten logits, and labels in batchs of X\n",
    "        # I have to compute the loss for each sample, and then average it\n",
    "        for i in range(batch_size):\n",
    "            # Get logits and labels for sample\n",
    "            logits = torch.tensor(logits_batch[i])\n",
    "            labels = torch.tensor(labels_batch[i])\n",
    "\n",
    "            # Compute loss\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels, reduction=\"mean\")\n",
    "\n",
    "            # Add to total loss\n",
    "            total_loss += loss\n",
    "\n",
    "        # Average loss\n",
    "        loss = total_loss / batch_size\n",
    "\n",
    "        # Clear everything!\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Print loss\n",
    "        return {'loss': loss}\n",
    "\n",
    "# Reduce it to 10 items (for debugging)\n",
    "eval_dataset_10 = eval_dataset.select(range(0, 10, 1))\n",
    "eval_dataset_20 = eval_dataset.select(range(10, 30, 1))\n",
    "eval_dataset_100 = eval_dataset.select(range(30, 130, 1))\n",
    "\n",
    "# Setup the model, use eval mode, and move it to GPU\n",
    "model = AutoModelForCausalLM.from_pretrained('codeparrot/codeparrot').to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     per_device_train_batch_size=2,  # batch size per device during training\n",
    "#     per_device_eval_batch_size=2,   # batch size for evaluation\n",
    "#     output_dir='./results',          # output directory\n",
    "#     num_train_epochs=3,              # total number of training epochs\n",
    "#     warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "#     weight_decay=0.01,               # strength of weight decay\n",
    "#     logging_dir='./logs',            # directory for storing logs\n",
    "# )\n",
    "\n",
    "# Initializing Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    "    # THESE THINGS DO NOT WORK????\n",
    "    # args=training_args,\n",
    ")\n",
    "\n",
    "# Because somehow the eval batching is happening for the full datasample i pass in\n",
    "# we need to do some form of manual batching ???\n",
    "def evaluate_dataset(in_dataset, batch_size=10):\n",
    "    # Lets get the dataset size\n",
    "    dataset_size = len(in_dataset)\n",
    "\n",
    "    # Lets get the number of batches\n",
    "    num_batches = dataset_size // batch_size\n",
    "\n",
    "    # the total loss\n",
    "    total_loss = 0\n",
    "\n",
    "    # Lets iterate over the batches\n",
    "    for i in range(num_batches):\n",
    "        # Get the start and end index\n",
    "        start_index = i * batch_size\n",
    "        end_index = (i + 1) * batch_size\n",
    "\n",
    "        # Get the batch\n",
    "        batch = in_dataset.select(range(start_index, end_index, 1))\n",
    "\n",
    "        # Perform evaluation\n",
    "        batch_res = trainer.evaluate(batch, metric_key_prefix=\"eval\")\n",
    "        batch_loss = float(batch_res[\"eval_loss\"])\n",
    "        total_loss += batch_loss\n",
    "        avg_loss = total_loss / (i + 1)\n",
    "\n",
    "        # Print loss\n",
    "        print(f\"Batch {i} - batch loss: {batch_loss} - avg loss: {avg_loss}   (start: {start_index}, end: {end_index})\")\n",
    "\n",
    "    # Garbage collection, to keep vram managable\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Return the average loss\n",
    "    return total_loss / num_batches\n",
    "\n",
    "# # Perform validation for first 10\n",
    "# print(\"=== 10 objects ===\")\n",
    "# output_10 = trainer.evaluate(eval_dataset_10)\n",
    "# print(\"> Output object ...\")\n",
    "# print(output_10)\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Perform validation for first 20\n",
    "# print(\"=== 20 objects ===\")\n",
    "# output_20 = evaluate_dataset(eval_dataset_20)\n",
    "# print(\"> Output object ...\")\n",
    "# print(output_20)\n",
    "\n",
    "# # Perform validation for first 100\n",
    "# print(\"=== 100 objects ===\")\n",
    "# output_100 = trainer.evaluate(eval_dataset_100)\n",
    "# print(\"> Output object ...\")\n",
    "# print(output_100)\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# Perform validation for all objects\n",
    "print(f\"=== All objects {len(eval_dataset)} ===\")\n",
    "output = evaluate_dataset(eval_dataset)\n",
    "print(\"> Final result ...\")\n",
    "print(\"loss = \", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rwkv-infctx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
